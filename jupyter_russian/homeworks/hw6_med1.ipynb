{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>\n",
    "Author: Yury Kashnitsky, Data Scientist at Mail.Ru Group\n",
    "\n",
    "This material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6\n",
    "### <center> Beating benchmarks in \"How good is your Medium article?\"\n",
    "    \n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"Assignment 6 baseline\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "        self.read = False\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if ('data-source', 'post_page') in attrs:\n",
    "            self.read = True\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'section':\n",
    "            self.read = False\n",
    "    def handle_data(self, d):\n",
    "        if self.read:\n",
    "            self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_and_write(inp_filename, is_train=True, max_iter=-1, total=-1):\n",
    "    \n",
    "    prefix = 'train' if is_train else 'test'\n",
    "    features = ['content', 'published', 'title', 'author']\n",
    "    feature_files = [data_file(f'{prefix}_{feat}.txt', mode='w') for feat in features]\n",
    "    \n",
    "    normalize_string = lambda s: str(s).replace('\\n', ' ').replace('\\r', '')\n",
    "    count = 0\n",
    "    \n",
    "    with data_file(inp_filename, mode='r') as inp_json_file:\n",
    "\n",
    "        for line in tqdm_notebook(inp_json_file, total=total):\n",
    "            if max_iter > 0:\n",
    "                max_iter -= 1\n",
    "            elif max_iter == 0:\n",
    "                break\n",
    "            json_data = read_json_line(line)\n",
    "            content = normalize_string(strip_tags(json_data['content']))\n",
    "            published = json_data['published']['$date']\n",
    "            title = normalize_string(json_data['title'])\n",
    "            author = normalize_string(json_data['author']['url'])\n",
    "            feature_data = [content, published, title, author]\n",
    "            for s, f in zip(feature_data, feature_files):\n",
    "                print(s, file=f)\n",
    "            count += 1\n",
    "                \n",
    "    for f in feature_files:\n",
    "        f.close()\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'hgiyma' # modify this if you need to\n",
    "def data_file(name, mode=None, csv=False, **kvargs):\n",
    "    path = os.path.join(PATH_TO_DATA, name)\n",
    "    if mode:\n",
    "        return open(path, mode=mode)\n",
    "    if csv:\n",
    "        return pd.read_csv(open(path), **kvargs)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4287c1d5d58d49339ae03b5b0b0df844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_total = extract_features_and_write('train.json', is_train=True, total=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850781868787447fb992951b66e0e3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_total = extract_features_and_write('test.json', is_train=False, total=34645)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "    - Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "    - Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_content.txt) = 62313\n",
      "len(train_published.txt) = 62313\n",
      "len(train_title.txt) = 62313\n",
      "len(train_author.txt) = 62313\n",
      "len(test_content.txt) = 34645\n",
      "len(test_published.txt) = 34645\n",
      "len(test_title.txt) = 34645\n",
      "len(test_author.txt) = 34645\n"
     ]
    }
   ],
   "source": [
    "for prefix in ['train', 'test']:\n",
    "    for feat in ['content', 'published', 'title', 'author']:\n",
    "        file_len = 0\n",
    "        file_name = f'{prefix}_{feat}.txt'\n",
    "        with open(data_file(file_name)) as file:\n",
    "            for _ in file:\n",
    "                file_len += 1\n",
    "        print(f'len({file_name}) = {file_len}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a134d7192ea41cba6c27e77cb5b464d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f770d23a2254851a83ecfd09bdfc15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 9min 37s, sys: 1min 51s, total: 11min 29s\n",
      "Wall time: 12min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "content_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=100000)\n",
    "with open(data_file('train_content.txt')) as f:\n",
    "    content_vectorizer.fit(tqdm_notebook(f, total=train_total, desc='fit()'))\n",
    "    f.seek(0)\n",
    "    X_train_content_sparse = content_vectorizer.transform(tqdm_notebook(f, total=train_total, desc='(train)'))\n",
    "with open(data_file('test_content.txt')) as f:\n",
    "    X_test_content_sparse = content_vectorizer.transform(tqdm_notebook(f, total=test_total, desc='(test)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c4a519fd9a41db9f74db6d3ca5a393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40d4011947c4ae29c255e40c1aa48b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0f9e14ea9f475f82cb4646d2cc4f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 7.23 s, sys: 424 ms, total: 7.65 s\n",
      "Wall time: 7.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=100000)\n",
    "with open(data_file('train_title.txt')) as f:\n",
    "    title_vectorizer.fit(tqdm_notebook(f, total=train_total, desc='fit()'))\n",
    "    f.seek(0)\n",
    "    X_train_title_sparse = title_vectorizer.fit_transform(tqdm_notebook(f, total=train_total, desc='(train)'))\n",
    "with open(data_file('test_title.txt')) as f:\n",
    "    X_test_title_sparse = title_vectorizer.transform(tqdm_notebook(f, total=test_total, desc='(test)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(name):\n",
    "    df = data_file(name, csv=True, header=None, parse_dates=[0])\n",
    "    time_series = df[0]\n",
    "    \n",
    "    hour_ohe = OneHotEncoder().fit(np.arange(24).reshape(-1,1))\n",
    "    hour = hour_ohe.transform(time_series.apply(lambda ts: ts.hour).values.reshape(-1,1))\n",
    "    \n",
    "    weekday_ohe = OneHotEncoder().fit(np.arange(7).reshape(-1,1))\n",
    "    weekday = hour_ohe.transform(time_series.apply(lambda ts: ts.weekday()).values.reshape(-1,1))\n",
    "    \n",
    "    month_ohe = OneHotEncoder().fit(np.arange(12).reshape(-1,1))\n",
    "    month = hour_ohe.transform(time_series.apply(lambda ts: ts.month).values.reshape(-1,1))\n",
    "    \n",
    "    year_ohe = OneHotEncoder().fit(np.arange(1970, 2020).reshape(-1,1))\n",
    "    year = year_ohe.transform(time_series.apply(lambda ts: ts.year).values.reshape(-1,1))\n",
    "    return hstack([year, month, weekday, hour])\n",
    "    \n",
    "X_train_time_features_sparse = time_features('train_published.txt')\n",
    "X_test_time_features_sparse = time_features('test_published.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrchnk/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/mrchnk/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 934 ms, sys: 38 ms, total: 972 ms\n",
      "Wall time: 982 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_author = data_file('train_author.txt', csv=True, header=None)\n",
    "test_author = data_file('test_author.txt', csv=True, header=None)\n",
    "author_label = LabelEncoder()\n",
    "author_ids = author_label.fit_transform(pd.concat([train_author, test_author]))\n",
    "author_ohe = OneHotEncoder()\n",
    "author_ohe.fit(author_ids.reshape(-1,1))\n",
    "\n",
    "X_train_author_sparse = author_ohe.transform(author_label.transform(train_author).reshape(-1,1))\n",
    "X_test_author_sparse = author_ohe.transform(author_label.transform(test_author).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join all sparse matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62313, 100000)\n",
      "(62313, 100000)\n",
      "(62313, 45374)\n",
      "(62313, 122)\n",
      "(34645, 100000)\n",
      "(34645, 100000)\n",
      "(34645, 45374)\n",
      "(34645, 122)\n"
     ]
    }
   ],
   "source": [
    "for f in [X_train_content_sparse, X_train_title_sparse, X_train_author_sparse, X_train_time_features_sparse]:\n",
    "    print(f.shape)\n",
    "    \n",
    "for f in [X_test_content_sparse, X_test_title_sparse, X_test_author_sparse, X_test_time_features_sparse]:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = csr_matrix(hstack([X_train_content_sparse, X_train_title_sparse,\n",
    "                                    X_train_author_sparse, X_train_time_features_sparse]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_sparse = csr_matrix(hstack([X_test_content_sparse, X_test_title_sparse,\n",
    "                                    X_test_author_sparse, X_test_time_features_sparse]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read train target and split data for validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62313,)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = data_file('train_log1p_recommends.csv', csv=True, index_col='id')\n",
    "y_train = train_target['log_recommends'].values\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a simple Ridge model and check MAE on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE=33\n",
    "N_JOBS=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 1.79 s, total: 1min 12s\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=33, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ridge.fit(X_train_part_sparse, y_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0757028267001929"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(ridge.predict(X_valid_sparse), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 1.32 s, total: 1min 28s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ridge.fit(X_train_sparse, y_train)\n",
    "ridge_test_pred = ridge.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename):\n",
    "    submission = data_file('sample_submission.csv', csv=True, index_col='id')\n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(data_file(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred, 'assignment6_medium_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now's the time for dirty Kaggle hacks. Form a submission file with all zeroes. Make a submission. What do you get if you think about it? How is it going to help you with modifying your predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_submission_file(np.zeros_like(ridge_test_pred), 'medium_all_zeros_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zmae = 4.33328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify predictions in an appropriate way (based on your all-zero submission) and make a new submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_pred_modif = ridge_test_pred + ( zmae - ridge_test_pred.mean() ) # You code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred_modif, \n",
    "                      'assignment6_medium_submission_with_hack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
